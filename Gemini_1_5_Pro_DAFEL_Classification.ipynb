{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bqMK4Xejpfj",
        "outputId": "70537269-a403-4d3a-f64e-846c46b4f500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_folder_path = \"/content/drive/My Drive/datasets/Classification/\"\n"
      ],
      "metadata": {
        "id": "Jz5riiU_j0ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Get list of all CSV files in the folder\n",
        "csv_files = glob.glob(os.path.join(csv_folder_path, \"*.csv\"))\n",
        "print(csv_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJAtIf0gosqI",
        "outputId": "e84428aa-cc36-4be0-fe83-a73169285134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/My Drive/datasets/Classification/balance-scale.csv', '/content/drive/My Drive/datasets/Classification/Contraceptive_Method_Classification.csv', '/content/drive/My Drive/datasets/Classification/eucalyptus_dataset.csv', '/content/drive/My Drive/datasets/Classification/heart .csv', '/content/drive/My Drive/datasets/Classification/breast-wisconsin.csv', '/content/drive/My Drive/datasets/Classification/blood-transfusion-sc.csv', '/content/drive/My Drive/datasets/Classification/pc1.csv', '/content/drive/My Drive/datasets/Classification/tic-tac-toe.csv', '/content/drive/My Drive/datasets/Classification/vehicle_c.csv', '/content/drive/My Drive/datasets/Classification/carPr.csv', '/content/drive/My Drive/datasets/Classification/credit_g.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier  # Changed to Classifier\n",
        "from sklearn.metrics import accuracy_score  # Using accuracy instead of RMSE\n",
        "import os\n",
        "import ast\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get free API key from https://aistudio.google.com/app/apikey\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")  # Replace with your key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Initialize Gemini model\n",
        "model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
        "\n",
        "class LLMFE:\n",
        "    def __init__(self, data, target_column, metric=accuracy_score, max_iter=50):\n",
        "        self.data = data\n",
        "        self.target = target_column\n",
        "        self.metric = metric\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "        # Split data\n",
        "        self.train, self.val = train_test_split(data, test_size=0.2, random_state=42)\n",
        "        self.X_train = self.train.drop(target_column, axis=1)\n",
        "        self.y_train = self.train[target_column]\n",
        "        self.X_val = self.val.drop(target_column, axis=1)\n",
        "        self.y_val = self.val[target_column]\n",
        "\n",
        "        # Memory buffers\n",
        "        self.memory = []\n",
        "        self.best_score = float('-inf')  # Changed to -inf since we want to maximize accuracy\n",
        "        self.best_transformation = None\n",
        "\n",
        "    def _create_prompt(self):\n",
        "      # ===== PHASE 1: FEATURE SELECTION =====\n",
        "      feature_selection_prompt = f\"\"\"\n",
        "      **PHASE 1: Feature Selection for '{self.target}'**\n",
        "\n",
        "      Dataset: {os.path.basename(self.data.attrs.get('filename', 'unknown'))}\n",
        "      Features: {list(self.X_train.columns)}\n",
        "\n",
        "      Instructions:\n",
        "      1. Analyze features using:\n",
        "      - Correlation (keep if |r| > 0.15)\n",
        "      - Domain knowledge (e.g., \"age\" matters for disease prediction)\n",
        "      - Variance (drop if >95% same value)\n",
        "      2. Flag redundant features (drop if correlation > 0.8 with more important feature)\n",
        "      3. For categorical features, check cardinality (drop if >20 unique values)\n",
        "\n",
        "      Required Output:\n",
        "      ```python\n",
        "      # Features to KEEP (high relevance):\n",
        "      keep = ['feature1', 'feature2']\n",
        "\n",
        "      # Features to DROP:\n",
        "      drop = ['id', 'constant_feature']\n",
        "\n",
        "      # Potential interaction terms:\n",
        "      interactions = [('feature1', 'feature2')]\n",
        "      ```\n",
        "      \"\"\"\n",
        "\n",
        "      # Dataset-specific guidance for classification tasks\n",
        "      dataset_guidance = {\n",
        "          \"balance-scale.csv\": {\n",
        "              \"hint\": \"Consider interactions between left and right weight/distance measurements.\",\n",
        "              \"examples\": [\n",
        "                  \"lambda df: df.assign(weight_diff=df['L-Weight'] - df['R-Weight'])\",\n",
        "                  \"lambda df: df.assign(distance_ratio=df['L-Distance'] / (df['R-Distance'] + 1e-9))\",\n",
        "                  \"lambda df: df.assign(total_torque=(df['L-Weight']*df['L-Distance']) - (df['R-Weight']*df['R-Distance']))\"\n",
        "              ]\n",
        "          },\n",
        "          \"Contraceptive_Method_Classification.csv\": {\n",
        "              \"hint\": \"Consider interactions between wife and husband characteristics and family size.\",\n",
        "              \"examples\": [\n",
        "                  \"lambda df: df.assign(education_gap=df['Wife Education'] - df['Husband Education'])\",\n",
        "                  \"lambda df: df.assign(children_per_year=df['Children'] / (df['Wife Age'] - 18))\",  # Approx years of fertility\n",
        "                  \"lambda df: df.assign(working_mom=df['Wife working'] * df['Children'])\"\n",
        "              ]\n",
        "          },\n",
        "          \"heart.csv\": {\n",
        "              \"hint\": \"Consider interactions between age, cholesterol levels, and other health indicators.\",\n",
        "              \"examples\": [\n",
        "                  \"lambda df: df.assign(age_chol_ratio=df['age'] / df['chol'])\",\n",
        "                  \"lambda df: df.assign(blood_pressure_diff=df['trestbps'] - 120)\",  # Difference from normal\n",
        "                  \"lambda df: df.assign(risk_factor=df['age'] * df['chol'] / (df['thalach'] + 1e-9))\"\n",
        "              ]\n",
        "          },\n",
        "          \"breast-wisconsin.csv\": {\n",
        "              \"hint\": \"Consider statistical properties of cell characteristics.\",\n",
        "              \"examples\": [\n",
        "                  \"lambda df: df.assign(mean_diff=df['mean radius'] - df['mean texture'])\",\n",
        "                  \"lambda df: df.assign(compactness=df['mean perimeter']**2 / (4*np.pi*df['mean area']))\",\n",
        "                  \"lambda df: df.assign(size_variation=df['mean area'] * df['mean smoothness'])\"\n",
        "              ]\n",
        "          },\n",
        "          \"tic-tac-toe.csv\": {\n",
        "              \"hint\": \"Consider patterns in board positions and winning configurations.\",\n",
        "              \"examples\": [\n",
        "                  \"lambda df: df.assign(top_row=df['top-left'] + df['top-middle'] + df['top-right'])\",\n",
        "                  \"lambda df: df.assign(diagonal_x=(df['top-left'] == df['middle-middle']) & (df['middle-middle'] == df['bottom-right']))\",\n",
        "                  \"lambda df: df.assign(center_control=df['middle-middle'].map({'x':1, 'o':-1, 'b':0}))\"\n",
        "              ]\n",
        "          },\n",
        "          \"vehicle_c.csv\": {\n",
        "              \"hint\": \"Consider geometric relationships between vehicle dimensions.\",\n",
        "              \"examples\": [\n",
        "                  \"lambda df: df.assign(compactness=df['COMPACTNESS'] * df['CIRCULARITY'])\",\n",
        "                  \"lambda df: df.assign(size_ratio=df['MAX.LENGTH_RECT'] / (df['MIN.LENGTH_RECT'] + 1e-9))\",\n",
        "                  \"lambda df: df.assign(scatter_radius=df['DISTANCE_CIRCULARITY'] * df['RADIUS_RATIO'])\"\n",
        "              ]\n",
        "          }\n",
        "      }\n",
        "\n",
        "      # Get filename for guidance lookup\n",
        "      filename = os.path.basename(self.data.attrs.get('filename', 'unknown'))\n",
        "\n",
        "      # Get dataset-specific guidance or use defaults\n",
        "      guidance = dataset_guidance.get(filename, {\n",
        "          \"hint\": \"Consider interactions between numerical features and encodings for categoricals.\",\n",
        "          \"examples\": [\n",
        "              \"lambda df: df.assign(feature1_squared=df['feature1'] ** 2)\",\n",
        "              \"lambda df: df.assign(feature_ratio=df['feature1'] / (df['feature2'] + 1e-9))\",\n",
        "              \"lambda df: df.assign(cat_encoded=df['category'].map({'A':1, 'B':2, 'C':3}))\"\n",
        "          ]\n",
        "      })\n",
        "\n",
        "      return f\"\"\"\n",
        "      You are a feature engineering expert working with a classification dataset to predict '{self.target}'.\n",
        "      The available features are: {list(self.X_train.columns)}\n",
        "\n",
        "      Dataset-specific guidance: {guidance['hint']}\n",
        "\n",
        "      Current best accuracy: {self.best_score:.4f}\n",
        "\n",
        "      Here are some relevant transformation examples for this dataset:\n",
        "      {chr(10).join(guidance['examples'])}\n",
        "\n",
        "      Please suggest 3 novel, computationally efficient feature transformations that would help classify '{self.target}'.\n",
        "      Focus on:\n",
        "      1. Creating meaningful interactions between features\n",
        "      2. Transformations that might reveal non-linear decision boundaries\n",
        "      3. Appropriate encodings for categorical variables\n",
        "\n",
        "      Format each transformation as a Python lambda function:\n",
        "      lambda df: df.assign(<new_feature_name>=<transformation_expression>)\n",
        "\n",
        "      Requirements:\n",
        "      1. Each transformation should be a single line\n",
        "      2. Avoid extremely complex expressions that might cause numerical instability\n",
        "      3. Include comments explaining the transformation when appropriate\n",
        "      \"\"\"\n",
        "\n",
        "    def generate_transformations(self):\n",
        "        prompt = self._create_prompt()\n",
        "        response = model.generate_content(prompt)\n",
        "        return self._parse_response(response.text)\n",
        "\n",
        "    def _parse_response(self, text):\n",
        "        transformations = []\n",
        "        for line in text.split('\\n'):\n",
        "            if line.startswith('lambda'):\n",
        "                try:\n",
        "                    ast.parse(line)\n",
        "                    transformations.append(line.strip())\n",
        "                except SyntaxError:\n",
        "                    continue\n",
        "        return transformations[:3]\n",
        "\n",
        "    def evaluate_transformation(self, transformation):\n",
        "        try:\n",
        "            func = eval(transformation)\n",
        "\n",
        "            # Apply transformation\n",
        "            X_train_trans = func(self.X_train.copy())\n",
        "            X_val_trans = func(self.X_val.copy())\n",
        "\n",
        "            # Encode categorical features\n",
        "            for col in X_train_trans.select_dtypes(include=['object']).columns:\n",
        "                X_train_trans[col], uniques = pd.factorize(X_train_trans[col])\n",
        "                X_val_trans[col] = X_val_trans[col].map({val: idx for idx, val in enumerate(uniques)}).fillna(-1)\n",
        "\n",
        "            # Train model - using Classifier instead of Regressor\n",
        "            model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "            model.fit(X_train_trans, self.y_train)\n",
        "            preds = model.predict(X_val_trans)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            score = self.metric(self.y_val, preds)\n",
        "\n",
        "            return {\n",
        "                'score': score,\n",
        "                'transformation': transformation,\n",
        "                'features': list(X_train_trans.columns)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in transformation: {e}\")\n",
        "            return None\n",
        "\n",
        "    def run(self):\n",
        "        for iteration in range(self.max_iter):\n",
        "            print(f\"\\n--- Iteration {iteration+1}/{self.max_iter} ---\")\n",
        "\n",
        "            # Generate transformations\n",
        "            transformations = self.generate_transformations()\n",
        "            print(f\"Generated {len(transformations)} transformations\")\n",
        "\n",
        "            # Evaluate transformations\n",
        "            results = [self.evaluate_transformation(t) for t in transformations]\n",
        "\n",
        "            # Update memory - now looking for higher scores\n",
        "            for result in results:\n",
        "                if result and result['score'] > self.best_score:\n",
        "                    self.best_score = result['score']\n",
        "                    self.best_transformation = result['transformation']\n",
        "                    self.memory.append(result)\n",
        "                    func = eval(self.best_transformation)\n",
        "                    self.X_train = func(self.X_train.copy())\n",
        "                    self.X_val = func(self.X_val.copy())\n",
        "\n",
        "            print(f\"Current Best Accuracy: {self.best_score:.6f}\")\n",
        "\n",
        "        return self.best_transformation, self.best_score"
      ],
      "metadata": {
        "id": "pN1sCtGluM15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming csv_files is defined somewhere in your code\n",
        "for csv_file in csv_files:\n",
        "    try:\n",
        "        print(f\"\\n📂 Processing file: {csv_file}\")\n",
        "        data = pd.read_csv(csv_file)\n",
        "\n",
        "        print(\"Columns in the dataset:\", list(data.columns))\n",
        "        target_col = input(\"👉 Please enter the name of the target column: \").strip()\n",
        "\n",
        "        if target_col not in data.columns:\n",
        "            print(f\"❌ Column '{target_col}' not found in dataset. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"🎯 Using target column: {target_col}\")\n",
        "\n",
        "        # Run the LLMFE pipeline for classification\n",
        "        fe = LLMFE(data=data, target_column=target_col, max_iter=3)\n",
        "        best_trans, best_score = fe.run()\n",
        "\n",
        "        print(f\"\\n✅ Done: {os.path.basename(csv_file)}\")\n",
        "        print(f\"🔍 Best Transformation: {best_trans}\")\n",
        "        print(f\"📈 Validation Accuracy: {best_score:.4f}\")  # Changed from MSE to Accuracy\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {csv_file}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eF-bPBWpn6of",
        "outputId": "5c40da9b-6ea8-421d-963a-e7d54991e39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📂 Processing file: /content/drive/My Drive/datasets/Classification/balance-scale.csv\n",
            "Columns in the dataset: ['Class', 'L-Weight', 'L-Distance', 'R-Weight', 'R-Distance']\n",
            "👉 Please enter the name of the target column: Class\n",
            "🎯 Using target column: Class\n",
            "\n",
            "--- Iteration 1/3 ---\n",
            "Generated 3 transformations\n",
            "Current Best Accuracy: 1.000000\n",
            "\n",
            "--- Iteration 2/3 ---\n",
            "Generated 3 transformations\n",
            "Current Best Accuracy: 1.000000\n",
            "\n",
            "--- Iteration 3/3 ---\n",
            "Generated 3 transformations\n",
            "Current Best Accuracy: 1.000000\n",
            "\n",
            "✅ Done: balance-scale.csv\n",
            "🔍 Best Transformation: lambda df: df.assign(Leverage_Diff=df['L-Weight'] * df['L-Distance'] - df['R-Weight'] * df['R-Distance']) # Captures the difference in leverage/torque between left and right sides.\n",
            "📈 Validation Accuracy: 1.0000\n",
            "\n",
            "📂 Processing file: /content/drive/My Drive/datasets/Classification/Contraceptive_Method_Classification.csv\n",
            "Columns in the dataset: ['Wife Age', 'Wife Education', 'Husband Education', 'Children', 'Wife religion', 'Wife working', 'Husband Occupation', 'SOLI', 'Media Exposure', 'Contraceptive Method']\n",
            "👉 Please enter the name of the target column: Contraceptive Method\n",
            "🎯 Using target column: Contraceptive Method\n",
            "\n",
            "--- Iteration 1/3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1343.34ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Error processing /content/drive/My Drive/datasets/Classification/Contraceptive_Method_Classification.csv: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "\n",
            "📂 Processing file: /content/drive/My Drive/datasets/Classification/eucalyptus_dataset.csv\n",
            "Columns in the dataset: ['Abbrev', 'Rep', 'Locality', 'Map_Ref', 'Latitude', 'Altitude', 'Rainfall', 'Frosts', 'Year', 'Sp', 'PMCno', 'DBH', 'Ht', 'Surv', 'Vig', 'Ins_res', 'Stem_Fm', 'Crown_Fm', 'Brnch_Fm', 'Utility']\n",
            "👉 Please enter the name of the target column: Utility\n",
            "🎯 Using target column: Utility\n",
            "\n",
            "--- Iteration 1/3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 3163.23ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Error processing /content/drive/My Drive/datasets/Classification/eucalyptus_dataset.csv: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "\n",
            "📂 Processing file: /content/drive/My Drive/datasets/Classification/heart .csv\n",
            "Columns in the dataset: ['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope', 'HeartDisease']\n",
            "👉 Please enter the name of the target column: HeartDisease\n",
            "🎯 Using target column: HeartDisease\n",
            "\n",
            "--- Iteration 1/3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1620.82ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Error processing /content/drive/My Drive/datasets/Classification/heart .csv: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "\n",
            "📂 Processing file: /content/drive/My Drive/datasets/Classification/breast-wisconsin.csv\n",
            "Columns in the dataset: ['Clump_Thickness', 'Cell_Size_Uniformity', 'Cell_Shape_Uniformity', 'Marginal_Adhesion', 'Single_Epi_Cell_Size', 'Bare_Nuclei', 'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses', 'Class']\n",
            "👉 Please enter the name of the target column: Class\n",
            "🎯 Using target column: Class\n",
            "\n",
            "--- Iteration 1/3 ---\n",
            "Generated 3 transformations\n",
            "Current Best Accuracy: 0.971429\n",
            "\n",
            "--- Iteration 2/3 ---\n",
            "Generated 3 transformations\n",
            "Current Best Accuracy: 0.978571\n",
            "\n",
            "--- Iteration 3/3 ---\n",
            "Generated 3 transformations\n",
            "Current Best Accuracy: 0.978571\n",
            "\n",
            "✅ Done: breast-wisconsin.csv\n",
            "🔍 Best Transformation: lambda df: df.assign(chromatin_nucleoli_mitoses_sum = df['Bland_Chromatin'] + df['Normal_Nucleoli'] + df['Mitoses'])\n",
            "📈 Validation Accuracy: 0.9786\n",
            "\n",
            "📂 Processing file: /content/drive/My Drive/datasets/Classification/blood-transfusion-sc.csv\n",
            "Columns in the dataset: ['V1', 'V2', 'V3', 'V4', 'Class']\n",
            "👉 Please enter the name of the target column: Class\n",
            "🎯 Using target column: Class\n",
            "\n",
            "--- Iteration 1/3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1038.61ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Error processing /content/drive/My Drive/datasets/Classification/blood-transfusion-sc.csv: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "\n",
            "📂 Processing file: /content/drive/My Drive/datasets/Classification/pc1.csv\n",
            "Columns in the dataset: ['loc', 'v(g)', 'ev(g)', 'iv(G)', 'N', 'V', 'L', 'D', 'I', 'E', 'B', 'T', 'lOCode', 'lOComment', 'locCodeAndComment', 'lOBlank', 'uniq_Op', 'uniq_Opnd', 'total_Op', 'total_Opnd', 'branchCount', 'defects']\n",
            "👉 Please enter the name of the target column: defects\n",
            "🎯 Using target column: defects\n",
            "\n",
            "--- Iteration 1/3 ---\n",
            "Generated 3 transformations\n",
            "Current Best Accuracy: 0.923423\n",
            "\n",
            "--- Iteration 2/3 ---\n",
            "Generated 3 transformations\n",
            "Current Best Accuracy: 0.923423\n",
            "\n",
            "--- Iteration 3/3 ---\n",
            "Generated 3 transformations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py:2153: RuntimeWarning: overflow encountered in cast\n",
            "  arr = np.asarray(values, dtype=dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in transformation: Input X contains infinity or a value too large for dtype('float32').\n",
            "Current Best Accuracy: 0.923423\n",
            "\n",
            "✅ Done: pc1.csv\n",
            "🔍 Best Transformation: lambda df: df.assign(complexity_volume = df['v(g)'] * df['loc'])\n",
            "📈 Validation Accuracy: 0.9234\n",
            "\n",
            "📂 Processing file: /content/drive/My Drive/datasets/Classification/tic-tac-toe.csv\n",
            "Columns in the dataset: ['top-left-square', 'top-middle-square', 'top-right-square', 'middle-left-square', 'middle-middle-square', 'middle-right-square', 'bottom-left-square', 'bottom-middle-square', 'bottom-right-square', 'Class']\n",
            "👉 Please enter the name of the target column: Class\n",
            "🎯 Using target column: Class\n",
            "\n",
            "--- Iteration 1/3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 912.79ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Error processing /content/drive/My Drive/datasets/Classification/tic-tac-toe.csv: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "\n",
            "📂 Processing file: /content/drive/My Drive/datasets/Classification/vehicle_c.csv\n",
            "Columns in the dataset: ['COMPACTNESS', 'CIRCULARITY', 'DISTANCE_CIRCULARITY', 'RADIUS_RATIO', 'PR.AXIS_ASPECT_RATIO', 'MAX.LENGTH_ASPECT_RATIO', 'SCATTER_RATIO', 'ELONGATEDNESS', 'PR.AXIS_RECTANGULARITY', 'MAX.LENGTH_RECTANGULARITY', 'SCALED_VARIANCE_MAJOR', 'SCALED_VARIANCE_MINOR', 'SCALED_RADIUS_OF_GYRATION', 'SKEWNESS_ABOUT_MAJOR', 'SKEWNESS_ABOUT_MINOR', 'KURTOSIS_ABOUT_MAJOR', 'KURTOSIS_ABOUT_MINOR', 'HOLLOWS_RATIO', 'Class']\n",
            "👉 Please enter the name of the target column: Class\n",
            "🎯 Using target column: Class\n",
            "\n",
            "--- Iteration 1/3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 809.43ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Error processing /content/drive/My Drive/datasets/Classification/vehicle_c.csv: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "\n",
            "📂 Processing file: /content/drive/My Drive/datasets/Classification/carPr.csv\n",
            "Columns in the dataset: ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
            "👉 Please enter the name of the target column: class\n",
            "🎯 Using target column: class\n",
            "\n",
            "--- Iteration 1/3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 937.02ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Error processing /content/drive/My Drive/datasets/Classification/carPr.csv: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "\n",
            "📂 Processing file: /content/drive/My Drive/datasets/Classification/credit_g.csv\n",
            "Columns in the dataset: ['checking_status', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings_status', 'employment', 'installment_commitment', 'personal_status', 'other_parties', 'residence_since', 'property_magnitude', 'age', 'other_payment_plans', 'housing', 'existing_credits', 'job', 'num_dependents', 'own_telephone', 'foreign_worker', 'class']\n",
            "👉 Please enter the name of the target column: class\n",
            "🎯 Using target column: class\n",
            "\n",
            "--- Iteration 1/3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 936.29ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Error processing /content/drive/My Drive/datasets/Classification/credit_g.csv: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0A1KCN6CoVQt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}